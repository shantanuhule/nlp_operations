{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1"
      ],
      "metadata": {
        "id": "AGr6yAe25Obf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6K1ZhiJwk2bh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e857b3c2-5367-4a6f-afc5-318c18a1902c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "whitespace_tokenizer = WhitespaceTokenizer()\n",
        "whitespace_tokens = whitespace_tokenizer.tokenize(sentence)\n",
        "\n",
        "print(\"Whitespace Tokenization:\")\n",
        "print(whitespace_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irF5z7bd1mKX",
        "outputId": "19dea55d-c502-41d5-9a32-eb689f44adb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whitespace Tokenization:\n",
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "punct_tokenizer = WordPunctTokenizer()\n",
        "punct_tokens = punct_tokenizer.tokenize(sentence)\n",
        "\n",
        "print(\"Punctuation-Based Tokenization:\")\n",
        "print(punct_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2Mzvdjm2ATW",
        "outputId": "7a1ebe63-bd64-4538-b0fa-8da457a66f37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Punctuation-Based Tokenization:\n",
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "treebank_tokenizer = TreebankWordTokenizer()\n",
        "treebank_tokens = treebank_tokenizer.tokenize(sentence)\n",
        "\n",
        "print(\"Treebank Tokenization:\")\n",
        "print(treebank_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3rFIDEc2Ezz",
        "outputId": "944fc58c-ab0c-4f8f-eafa-42e248d71975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treebank Tokenization:\n",
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "tweet_tokens = tweet_tokenizer.tokenize(sentence)\n",
        "\n",
        "print(\"Tweet Tokenization:\")\n",
        "print(tweet_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAPlqU1g2g-h",
        "outputId": "b89d6086-db88-4e4f-8f33-cc4a239121f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet Tokenization:\n",
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "\n",
        "mwe_tokenizer = MWETokenizer()\n",
        "mwe_tokens = mwe_tokenizer.tokenize(sentence.split())\n",
        "\n",
        "print(\"MWE Tokenization:\")\n",
        "print(mwe_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILiSBHYO3oq_",
        "outputId": "60fb43c9-7790-45e2-ce80-3472b3423d8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MWE Tokenization:\n",
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "\n",
        "stemmers = [PorterStemmer(), SnowballStemmer(\"english\")]\n",
        "\n",
        "print(\"Stemming:\")\n",
        "for stemmer in stemmers:\n",
        "    stemmed_words = [stemmer.stem(word) for word in treebank_tokens]\n",
        "    print(stemmer.__class__.__name__ + \":\")\n",
        "    print(stemmed_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-ozelQf4OMW",
        "outputId": "49c9ff18-ec3e-4428-8157-edc39a4137ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming:\n",
            "PorterStemmer:\n",
            "['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazi', 'dog', '.']\n",
            "SnowballStemmer:\n",
            "['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazi', 'dog', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(\"Lemmatization:\")\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in treebank_tokens]\n",
        "print(lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sweLZyg04sS9",
        "outputId": "1922ae6b-cff9-4a48-d186-59171827b7b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatization:\n",
            "['The', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazy', 'dog', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2"
      ],
      "metadata": {
        "id": "P0PcdCCl6TOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n"
      ],
      "metadata": {
        "id": "k7iweSLM5JLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "data = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"The dog barks loudly.\",\n",
        "    \"The cat sleeps lazily.\",\n",
        "    \"The fox and the dog are friends.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "LCPl5L7z6Sva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bag-of-Words Approach - Count Occurrence:\n",
        "# Tokenize the data\n",
        "tokenized_data = [word_tokenize(sentence) for sentence in data]\n",
        "\n",
        "# Initialize the CountVectorizer\n",
        "count_vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the tokenized data\n",
        "count_occurrence = count_vectorizer.fit_transform([\" \".join(tokens) for tokens in tokenized_data])\n",
        "\n",
        "# Get the feature names (words)\n",
        "feature_names = count_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print the count occurrence matrix\n",
        "print(\"Bag-of-Words - Count Occurrence:\")\n",
        "print(count_occurrence.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7u7QAiPJ6ZIo",
        "outputId": "61e9cf75-3ca2-4b02-9a28-0d35b3eadd76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag-of-Words - Count Occurrence:\n",
            "[[0 0 0 1 0 1 1 0 1 0 1 0 1 1 0 2]\n",
            " [0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1]\n",
            " [0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1]\n",
            " [1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the normalized count occurrence\n",
        "normalized_count_occurrence = count_occurrence / count_occurrence.sum(axis=1)\n",
        "\n",
        "# Print the normalized count occurrence matrix\n",
        "print(\"Bag-of-Words - Normalized Count Occurrence:\")\n",
        "print(normalized_count_occurrence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HleCLCqt7I23",
        "outputId": "b381b545-0b30-45c1-bf5b-502ab69922e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag-of-Words - Normalized Count Occurrence:\n",
            "[[0.         0.         0.         0.11111111 0.         0.11111111\n",
            "  0.11111111 0.         0.11111111 0.         0.11111111 0.\n",
            "  0.11111111 0.11111111 0.         0.22222222]\n",
            " [0.         0.         0.25       0.         0.         0.25\n",
            "  0.         0.         0.         0.         0.         0.25\n",
            "  0.         0.         0.         0.25      ]\n",
            " [0.         0.         0.         0.         0.25       0.\n",
            "  0.         0.         0.         0.25       0.         0.\n",
            "  0.         0.         0.25       0.25      ]\n",
            " [0.14285714 0.14285714 0.         0.         0.         0.14285714\n",
            "  0.14285714 0.14285714 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.28571429]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the tokenized data\n",
        "tfidf = tfidf_vectorizer.fit_transform([\" \".join(tokens) for tokens in tokenized_data])\n",
        "\n",
        "# Print the TF-IDF matrix\n",
        "print(\"TF-IDF:\")\n",
        "print(tfidf.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWVnJlU58i7h",
        "outputId": "2f2731b9-f11b-461f-908e-ada047a4961e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF:\n",
            "[[0.         0.         0.         0.37481119 0.         0.23923713\n",
            "  0.29550545 0.         0.37481119 0.         0.37481119 0.\n",
            "  0.37481119 0.37481119 0.         0.39118406]\n",
            " [0.         0.         0.61087812 0.         0.         0.38991559\n",
            "  0.         0.         0.         0.         0.         0.61087812\n",
            "  0.         0.         0.         0.31878155]\n",
            " [0.         0.         0.         0.         0.55280532 0.\n",
            "  0.         0.         0.         0.55280532 0.         0.\n",
            "  0.         0.         0.55280532 0.28847675]\n",
            " [0.44201611 0.44201611 0.         0.         0.         0.28213316\n",
            "  0.34849058 0.44201611 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.46132469]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Word2Vec model\n",
        "word2vec_model = Word2Vec(tokenized_data, size=100, window=5, min_count=1)\n",
        "\n",
        "# Get the word embeddings\n",
        "word_embeddings = word2vec_model.wv\n",
        "\n",
        "# Print the word embeddings for each word\n",
        "print(\"Word Embeddings:\")\n",
        "for word in feature_names:\n",
        "    print(word, \":\", word_embeddings[word])\n"
      ],
      "metadata": {
        "id": "D2Z-MKPw92ta"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}